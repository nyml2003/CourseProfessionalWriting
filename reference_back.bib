@misc{Yao2024,
   abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.},
   author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
   doi = {10.1016/j.hcc.2024.100211},
   issn = {26672952},
   issue = {2},
   journal = {High-Confidence Computing},
   keywords = {ChatGPT,LLM attacks,LLM privacy,LLM security,LLM vulnerabilities,Large Language Model (LLM)},
   month = {6},
   publisher = {Shandong University},
   title = {A Survey on Large Language Model (LLM) Security and Privacy: The Good, The Bad, and The Ugly},
   volume = {4},
   year = {2024},
}
@inproceedings{Fan2024,
   abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/},
   author = {Wenqi Fan and Yujuan Ding and Liangbo Ning and Shijie Wang and Hengyun Li and Dawei Yin and Tat Seng Chua and Qing Li},
   doi = {10.1145/3637528.3671470},
   isbn = {9798400704901},
   issn = {2154817X},
   booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {fine-tuning,in-context learning,large language model (llm),pre-training,prompting,retrieval augmented generation (rag)},
   month = {8},
   pages = {6491-6501},
   publisher = {Association for Computing Machinery},
   title = {A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models},
   year = {2024},
}
@article{Yang2024,
   abstract = {While Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), it also presents challenges that can affect model accuracy and performance. Practical applications show that RAG can mask the intrinsic capabilities of LLMs. Firstly, LLMs may become overly dependent on external retrieval, underutilizing their own knowledge and inference abilities, which can reduce responsiveness. Secondly, RAG techniques might introduce irrelevant or low-quality information, adding noise to the LLM. This can disrupt the normal generation process, leading to inefficient and low-quality content, especially when dealing with complex problems. This paper proposes a RAG framework that uses reflective tags to control retrieval. This framework evaluates retrieved documents in parallel and incorporates the Chain of Thought (CoT) technique for step-by-step content generation. The model selects the highest quality and most accurate content for final generation. The main contributions include: 1) Reducing the hallucination problem by selectively utilizing high-scoring document, 2) Enhancing real-time performance through timely external database retrieval, and 3) Minimizing negative impacts by filtering out irrelevant or unreliable information through parallel content generation and reflective tagging. These advancements aim to optimize the integration of retrieval mechanisms with LLMs, ensuring high-quality and reliable outputs.},
   author = {Chengyuan Yang and Satoshi Fujita},
   doi = {10.20944/preprints202408.2152.v1},
   keywords = {chain of thought,large language models,reflective tag,retrieval-augmented generation},
   title = {Adaptive Control of Retrieval-Augmented Generation for LLMs Through Reflective Tags},
   url = {www.preprints.org},
   year = {2024},
}
@article{Posedaru2024,
   abstract = {The article studies the current text processing tools based on Artificial Intelligence. A literature review is done emphasizing the dynamic evolution of AI-powered text analytics, having as its central tool ChatGPT and its capabilities. The focus is centered on the techniques and methods that are using embeddings in order to improve large language models (LLMs).In this paper is analyzed the current situation of the literature in terms of text processing using Retrieval-Augmented Generation and is highlighted the potential of this technology to enhance the interpretability and trust in applications critical, such as those related to education or business. AI has revolutionized natural language processing (NLP), which facilitated the machines to interpret and generate text efficiently and accurately. In addition, large language models with external knowledge bases have been developed. These are used to produce more accurate and contextually relevant text responses. This approach is called Retrieval-Augmented Generation (RAG is one of the most significant recent advancements in this field.Based on our study, two use cases are implemented to show the applicability of our study: one related to education and one related to business IT-related documents. The methodology describes the techniques used. This includes retrieval-augmented generation and embedding stored using vector databases. Our custom models are evaluated by comparing them to the general ones, without embeddings, showing superior performance.The article highlights remarkable progress in Retrieval-Augmented Generation (RAG), which is used for AI text processing with a focus on business and education fields. Further in this paper, many of the most significant highlights are presented, which include a scalable framework for AI applications, a new integration of Retrieval-Augmented Generation and embeddings, practical application demonstrations, bridging gaps in the analysis op AI text, significant development in AI performance and optimizing educational and business processes.},
   author = {Bogdan-Stefan Posedaru and Florin-Valeriu Pantelimon and Mihai-Nicolae Dulgheru and Tiberiu-Marian Georgescu},
   doi = {10.2478/picbe-2024-0018},
   issue = {1},
   journal = {Proceedings of the International Conference on Business Excellence},
   month = {6},
   pages = {209-222},
   publisher = {Walter de Gruyter GmbH},
   title = {Artificial Intelligence Text Processing Using Retrieval-Augmented Generation: Applications in Business and Education Fields},
   volume = {18},
   year = {2024},
}
@article{Wiratunga2024,
   abstract = {Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.},
   author = {Nirmalie Wiratunga and Ramitha Abeyratne and Lasal Jayawardena and Kyle Martin and Stewart Massie and Ikechukwu Nkisi-Orji and Ruvan Weerasinghe and Anne Liret and Bruno Fleisch},
   month = {4},
   title = {CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering},
   url = {http://arxiv.org/abs/2404.04302},
   year = {2024},
}
@misc{ke2024developmenttestingretrievalaugmented,
      title={Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report}, 
      author={YuHe Ke and Liyuan Jin and Kabilan Elangovan and Hairil Rizal Abdullah and Nan Liu and Alex Tiong Heng Sia and Chai Rick Soh and Joshua Yi Min Tung and Jasmine Chiat Ling Ong and Daniel Shu Wei Ting},
      year={2024},
      eprint={2402.01733},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01733}, 
}
@misc{li2024enhancingllmfactualaccuracy,
      title={Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases}, 
      author={Jiarui Li and Ye Yuan and Zehua Zhang},
      year={2024},
      eprint={2403.10446},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.10446}, 
}
@article{Daneshvar2024,
   abstract = {Detecting vulnerabilities is a crucial task for maintaining the integrity, availability, and security of software systems. Utilizing DL-based models for vulnerability detection has become commonplace in recent years. However, such deep learning-based vulnerability detectors (DLVD) suffer from a shortage of sizable datasets to train effectively. Data augmentation can potentially alleviate the shortage of data, but augmenting vulnerable code is challenging and requires designing a generative solution that maintains vulnerability. Hence, the work on generating vulnerable code samples has been limited and previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities. Lately, large language models (LLMs) are being used for solving various code generation and comprehension tasks and have shown inspiring results, especially when fused with retrieval augmented generation (RAG). In this study, we explore three different strategies to augment vulnerabilities both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. We conducted an extensive evaluation of our proposed approach on three vulnerability datasets and three DLVD models, using two LLMs. Our results show that our injection-based clustering-enhanced RAG method beats the baseline setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling (ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with 15K generated vulnerable samples. Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.},
   author = {Seyed Shayan Daneshvar and Yu Nong and Xu Yang and Shaowei Wang and Haipeng Cai},
   doi = {10.1145/3676961},
   month = {8},
   title = {Exploring RAG-based Vulnerability Augmentation with LLMs},
   url = {http://arxiv.org/abs/2408.04125 http://dx.doi.org/10.1145/3676961},
   year = {2024},
}
@article{He2024,
   abstract = {Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\footnote\{Our codes and datasets are available at: \url\{https://github.com/XiaoxinHe/G-Retriever\}\}},
   author = {Xiaoxin He and Yijun Tian and Yifei Sun and Nitesh V. Chawla and Thomas Laurent and Yann LeCun and Xavier Bresson and Bryan Hooi},
   month = {2},
   title = {G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering},
   url = {http://arxiv.org/abs/2402.07630},
   year = {2024},
}
@misc{siriwardhana2022improvingdomainadaptationretrieval,
      title={Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering}, 
      author={Shamane Siriwardhana and Rivindu Weerasekera and Elliott Wen and Tharindu Kaluarachchi and Rajib Rana and Suranga Nanayakkara},
      year={2022},
      eprint={2210.02627},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.02627}, 
}
@misc{Miao2024,
   abstract = {The integration of large language models (LLMs) into healthcare, particularly in nephrology, represents a significant advancement in applying advanced technology to patient care, medical research, and education. These advanced models have progressed from simple text processors to tools capable of deep language understanding, offering innovative ways to handle health-related data, thus improving medical practice efficiency and effectiveness. A significant challenge in medical applications of LLMs is their imperfect accuracy and/or tendency to produce hallucinations—outputs that are factually incorrect or irrelevant. This issue is particularly critical in healthcare, where precision is essential, as inaccuracies can undermine the reliability of these models in crucial decision-making processes. To overcome these challenges, various strategies have been developed. One such strategy is prompt engineering, like the chain-of-thought approach, which directs LLMs towards more accurate responses by breaking down the problem into intermediate steps or reasoning sequences. Another one is the retrieval-augmented generation (RAG) strategy, which helps address hallucinations by integrating external data, enhancing output accuracy and relevance. Hence, RAG is favored for tasks requiring up-to-date, comprehensive information, such as in clinical decision making or educational applications. In this article, we showcase the creation of a specialized ChatGPT model integrated with a RAG system, tailored to align with the KDIGO 2023 guidelines for chronic kidney disease. This example demonstrates its potential in providing specialized, accurate medical advice, marking a step towards more reliable and efficient nephrology practices.},
   author = {Jing Miao and Charat Thongprayoon and Supawadee Suppadungsuk and Oscar A. Garcia Valencia and Wisit Cheungpasitporn},
   doi = {10.3390/medicina60030445},
   issn = {16489144},
   issue = {3},
   journal = {Medicina (Lithuania)},
   keywords = {artificial intelligence,chronic kidney disease,large language models (LLMs),nephrology,retrieval-augmented generation (RAG)},
   month = {3},
   pmid = {38541171},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Integrating Retrieval-Augmented Generation with Large Language Models in Nephrology: Advancing Practical Applications},
   volume = {60},
   year = {2024},
}
@article{Zhang2024,
   abstract = {With the advance of artificial intelligence (AI), the emergence of Google Gemini and OpenAI Q* marks the direction towards artificial general intelligence (AGI). To implement AGI, the concept of interactive AI (IAI) has been introduced, which can interactively understand and respond not only to human user input but also to dynamic system and network conditions. In this article, we explore an integration and enhancement of IAI in networking. We first comprehensively review recent developments and future perspectives of AI and then introduce the technology and components of IAI. We then explore the integration of IAI into the next-generation networks, focusing on how implicit and explicit interactions can enhance network functionality, improve user experience, and promote efficient network management. Subsequently, we propose an IAI-enabled network management and optimization framework, which consists of environment, perception, action, and brain units. We also design the pluggable large language model (LLM) module and retrieval augmented generation (RAG) module to build the knowledge base and contextual memory for decision-making in the brain unit. We demonstrate the effectiveness of the framework through case studies. Finally, we discuss potential research directions for IAI-based networks.},
   author = {Ruichen Zhang and Hongyang Du and Yinqiu Liu and Dusit Niyato and Jiawen Kang and Sumei Sun and Xuemin Shen and H. Vincent Poor},
   month = {1},
   title = {Interactive AI with Retrieval-Augmented Generation for Next Generation Networking},
   url = {http://arxiv.org/abs/2401.11391},
   year = {2024},
}
@misc{li2024laragenhancingllmbasedasraccuracy,
      title={LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented Generation}, 
      author={Shaojun Li and Hengchao Shang and Daimeng Wei and Jiaxin Guo and Zongyao Li and Xianghui He and Min Zhang and Hao Yang},
      year={2024},
      eprint={2409.08597},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2409.08597}, 
}
@article{Wu2024,
   abstract = {Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches. This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.},
   author = {Mingrui Wu and Sheng Cao},
   month = {4},
   title = {LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding},
   url = {http://arxiv.org/abs/2404.05825},
   year = {2024},
}
@misc{wang2024llmsknowneedleveraging,
      title={LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation}, 
      author={Keheng Wang and Feiyu Duan and Peiguang Li and Sirui Wang and Xunliang Cai},
      year={2024},
      eprint={2404.14043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14043}, 
}
@inproceedings{Kuppa2024,
   abstract = {The emergence of large language models (LLMs) has revolutionized the field of AI, introducing a new era of generative models applied across diverse use cases. Within this evolving AI application ecosystem, numerous stakeholders, including LLM and AI application service providers, use these models to cater to user needs. A significant challenge arises due to the need for more visibility and understanding of the inner workings of these models to end-users. This lack of transparency can lead to concerns about how the models are being used, how outputs are generated, the nature of the data they are trained on, and the potential biases they may harbor. The user trust becomes a critical aspect of deploying and managing these advanced AI applications. This paper highlights the safety and integrity issues associated with service providers who may introduce covert, unsafe policies into their systems. Our study focuses on two attacks: the injection of biased content in generative AI search services, and the manipulation of LLM outputs during inference by altering attention heads. Through empirical experiments, we show that malicious service providers can covertly inject malicious content into the outputs generated by LLMs without the awareness of the end-user. This study reveals the subtle yet significant ways LLM outputs can be compromised, highlighting the importance of vigilance and advanced security measures in AI-driven applications. We demonstrate empirically that is it possible to increase the citation score of LLM output to include erroneous or unnecessary sources of information to redirect a reader to a desired source of information.},
   author = {Aditya Kuppa and Jack Nicholls and Nhien An Le-Khac},
   doi = {10.5220/0012803100003767},
   isbn = {9789897587092},
   issn = {21847711},
   booktitle = {Proceedings of the International Conference on Security and Cryptography},
   keywords = {Adversarial,Generative AI,LLM,Security,Service Providers},
   pages = {777-785},
   publisher = {Science and Technology Publications, Lda},
   title = {Manipulating Prompts and Retrieval-Augmented Generation for LLM Service Providers},
   year = {2024},
}
@article{https://doi.org/10.1111/liv.15974,
author = {Giuffrè, Mauro and Kresevic, Simone and Pugliese, Nicola and You, Kisung and Shung, Dennis L.},
title = {Optimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes},
journal = {Liver International},
volume = {44},
number = {9},
pages = {2114-2124},
keywords = {ChatGPT, Fine-tuning, Large Language Models, LLMs, Retrieval Augmented Generation, Supervised Fine-Tuning, RHLF, Reinforcement Learning from Human Feedback, In-context Learning},
doi = {https://doi.org/10.1111/liv.15974},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/liv.15974},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/liv.15974},
abstract = {Abstract Large Language Models (LLMs) are transformer-based neural networks with billions of parameters trained on very large text corpora from diverse sources. LLMs have the potential to improve healthcare due to their capability to parse complex concepts and generate context-based responses. The interest in LLMs has not spared digestive disease academics, who have mainly investigated foundational LLM accuracy, which ranges from 25\% to 90\% and is influenced by the lack of standardized rules to report methodologies and results for LLM-oriented research. In addition, a critical issue is the absence of a universally accepted definition of accuracy, varying from binary to scalar interpretations, often tied to grader expertise without reference to clinical guidelines. We address strategies and challenges to increase accuracy. In particular, LLMs can be infused with domain knowledge using Retrieval Augmented Generation (RAG) or Supervised Fine-Tuning (SFT) with reinforcement learning from human feedback (RLHF). RAG faces challenges with in-context window limits and accurate information retrieval from the provided context. SFT, a deeper adaptation method, is computationally demanding and requires specialized knowledge. LLMs may increase patient quality of care across the field of digestive diseases, where physicians are often engaged in screening, treatment and surveillance for a broad range of pathologies for which in-context learning or SFT with RLHF could improve clinical decision-making and patient outcomes. However, despite their potential, the safe deployment of LLMs in healthcare still needs to overcome hurdles in accuracy, suggesting a need for strategies that integrate human feedback with advanced model training.},
year = {2024}
}


@misc{hu2024promptperturbationretrievalaugmentedgeneration,
      title={Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models}, 
      author={Zhibo Hu and Chen Wang and Yanfeng Shu and Helen and Paik and Liming Zhu},
      year={2024},
      eprint={2402.07179},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.07179}, 
}
@misc{hu2024ragrausurveyretrievalaugmented,
      title={RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing}, 
      author={Yucheng Hu and Yuxing Lu},
      year={2024},
      eprint={2404.19543},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.19543}, 
}
@misc{destefano2024ragrollendtoendevaluation,
      title={Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks}, 
      author={Gianluca De Stefano and Lea Schönherr and Giancarlo Pellegrino},
      year={2024},
      eprint={2408.05025},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2408.05025}, 
}
@article{Phan2024,
   abstract = {Large Language Models (LLMs) have been applied to many research problems across various domains. One of the applications of LLMs is providing question-answering systems that cater to users from different fields. The effectiveness of LLM-based question-answering systems has already been established at an acceptable level for users posing questions in popular and public domains such as trivia and literature. However, it has not often been established in niche domains that traditionally require specialized expertise. To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance of three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering questions originating from Environmental Impact Statements prepared by U.S. federal government agencies in accordance with the National Environmental Environmental Act (NEPA). We specifically measure the ability of LLMs to understand the nuances of legal, technical, and compliance-related information present in NEPA documents in different contextual scenarios. For example, we test the LLMs' internal prior NEPA knowledge by providing questions without any context, as well as assess how LLMs synthesize the contextual information present in long NEPA documents to facilitate the question/answering task. We compare the performance of the long context LLMs and RAG powered models in handling different types of questions (e.g., problem-solving, divergent). Our results suggest that RAG powered models significantly outperform the long context models in the answer accuracy regardless of the choice of the frontier LLM. Our further analysis reveals that many models perform better answering closed questions than divergent and problem-solving questions.},
   author = {Hung Phan and Anurag Acharya and Sarthak Chaturvedi and Shivam Sharma and Mike Parker and Dan Nally and Ali Jannesari and Karl Pazdernik and Mahantesh Halappanavar and Sai Munikoti and Sameera Horawalavithana},
   month = {7},
   title = {RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension},
   url = {http://arxiv.org/abs/2407.07321},
   year = {2024},
}
@article{Zhao2024,
   abstract = {Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.},
   author = {Siyun Zhao and Yuqing Yang and Zilong Wang and Zhiyuan He and Luna K. Qiu and Lili Qiu},
   month = {9},
   title = {Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely},
   url = {http://arxiv.org/abs/2409.14924},
   year = {2024},
}
@misc{li2024retrievalaugmentedgenerationlongcontext,
      title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach}, 
      author={Zhuowan Li and Cheng Li and Mingyang Zhang and Qiaozhu Mei and Michael Bendersky},
      year={2024},
      eprint={2407.16833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.16833}, 
}
@article{Lewis2020,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   month = {5},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   url = {http://arxiv.org/abs/2005.11401},
   year = {2020},
}

@misc{guu2020realmretrievalaugmentedlanguagemodel,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08909}, 
}
@inproceedings{Xu2024,
   abstract = {In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.},
   author = {Zhentao Xu and Mark Jerome Cruz and Matthew Guevara and Tie Wang and Manasi Deshpande and Xiaofeng Wang and Zheng Li},
   doi = {10.1145/3626772.3661370},
   isbn = {9798400704314},
   booktitle = {SIGIR 2024 - Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {knowledge graph,large language model,question answering,retrieval-augmented generation},
   month = {7},
   pages = {2905-2909},
   publisher = {Association for Computing Machinery, Inc},
   title = {Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering},
   year = {2024},
}
@article{Ahn2022,
   abstract = {Users on the internet usually have conversations on interesting facts or topics along with diverse knowledge from the web. However, most existing knowledge-grounded conversation models consider only a single document regarding the topic of a conversation. The recently proposed retrieval-augmented models generate a response based on multiple documents; however, they ignore the given topic and use only the local context of the conversation. To this end, we introduce a novel retrieval-augmented response generation model that retrieves an appropriate range of documents relevant to both the topic and local context of a conversation and uses them for generating a knowledge-grounded response. Our model first accepts both topic words extracted from the whole conversation and the tokens before the response to yield multiple representations. It then chooses representations of the first N token and ones of keywords from the conversation and document encoders and compares the two groups of representation from the conversation with those groups of the document, respectively. For training, we introduce a new data-weighting scheme to encourage the model to produce knowledge-grounded responses without ground truth knowledge. Both automatic and human evaluation results with a large-scale dataset show that our models can generate more knowledgeable, diverse, and relevant responses compared to the state-of-the-art models.},
   author = {Yeonchan Ahn and Sang Goo Lee and Junho Shim and Jaehui Park},
   doi = {10.1109/ACCESS.2022.3228964},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Conversation,knowledge retrieval,knowledge-grounded conversation},
   pages = {131374-131385},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Retrieval-Augmented Response Generation for Knowledge-Grounded Conversation in the Wild},
   volume = {10},
   year = {2022},
}
@article{Zhang2023,
   abstract = {Large language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios. In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMs' feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling. These optimization strategies contribute to the outstanding empirical performance of the LLM-Embedder. Notably, it yields remarkable enhancements in retrieval augmentation for LLMs, surpassing both general-purpose and task-specific retrievers in various evaluation scenarios. Our checkpoint and source code are publicly available at https://github.com/FlagOpen/FlagEmbedding.},
   author = {Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},
   month = {10},
   title = {Retrieve Anything To Augment Large Language Models},
   url = {http://arxiv.org/abs/2310.07554},
   year = {2023},
}
@misc{Glass1949,
   abstract = {Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [ENTITY, SLOT, ?], a system is asked to 'fill' the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation , and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models 1 .},
   author = {Michael Glass and Gaetano Rossiello and Faisal Mahbub Chowdhury and Alfio Gliozzo},
   pages = {1939},
   publisher = {Association for Computational Linguistics},
   title = {Robust Retrieval Augmented Generation for Zero-shot Slot Filling},
   url = {https://github.com/facebookresearch/},
   year = {1949},
}
@article{Chan2024,
   abstract = {Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.},
   author = {Chi-Min Chan and Chunpu Xu and Ruibin Yuan and Hongyin Luo and Wei Xue and Yike Guo and Jie Fu},
   month = {3},
   title = {RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation},
   url = {http://arxiv.org/abs/2404.00610},
   year = {2024},
}
@article{Laban2024,
   abstract = {LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific \textit\{insights\} repeat across documents. The "Summary of a Haystack" (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects - Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56\%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.},
   author = {Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu},
   month = {7},
   title = {Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems},
   url = {http://arxiv.org/abs/2407.01370},
   year = {2024},
}
@article{Fatehkia2024,
   abstract = {Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.},
   author = {Masoomali Fatehkia and Ji Kim Lucas and Sanjay Chawla},
   month = {2},
   title = {T-RAG: Lessons from the LLM Trenches},
   url = {http://arxiv.org/abs/2402.07483},
   year = {2024},
}
@article{Yilma2024,
   abstract = {Large Language Models (LLMs) have immense potential to transform the telecommunications industry. They could help professionals understand complex standards, generate code, and accelerate development. However, traditional LLMs struggle with the precision and source verification essential for telecom work. To address this, specialized LLM-based solutions tailored to telecommunication standards are needed. Retrieval-augmented generation (RAG) offers a way to create precise, fact-based answers. This paper proposes TelecomRAG, a framework for a Telecommunication Standards Assistant that provides accurate, detailed, and verifiable responses. Our implementation, using a knowledge base built from 3GPP Release 16 and Release 18 specification documents, demonstrates how this assistant surpasses generic LLMs, offering superior accuracy, technical depth, and verifiability, and thus significant value to the telecommunications field.},
   author = {Girma M. Yilma and Jose A. Ayala-Romero and Andres Garcia-Saavedra and Xavier Costa-Perez},
   month = {6},
   title = {TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs},
   url = {http://arxiv.org/abs/2406.07053},
   year = {2024},
}
@article{Zeng2024,
   abstract = {Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.},
   author = {Shenglai Zeng and Jiankun Zhang and Pengfei He and Yue Xing and Yiding Liu and Han Xu and Jie Ren and Shuaiqiang Wang and Dawei Yin and Yi Chang and Jiliang Tang},
   month = {2},
   title = {The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)},
   url = {http://arxiv.org/abs/2402.16893},
   year = {2024},
}
@inproceedings{Cuconasu2024,
   abstract = {Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.},
   author = {Florin Cuconasu and Giovanni Trappolini and Federico Siciliano and Simone Filice and Cesare Campagnano and Yoelle Maarek and Nicola Tonellotto and Fabrizio Silvestri},
   doi = {10.1145/3626772.3657834},
   isbn = {9798400704314},
   booktitle = {SIGIR 2024 - Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {information retrieval,llm,rag},
   month = {7},
   pages = {719-729},
   publisher = {Association for Computing Machinery, Inc},
   title = {The Power of Noise: Redefining Retrieval for RAG Systems},
   year = {2024},
}
@article{Cheng2024,
   abstract = {Large language models (LLMs) have raised concerns about potential security threats despite performing significantly in Natural Language Processing (NLP). Backdoor attacks initially verified that LLM is doing substantial harm at all stages, but the cost and robustness have been criticized. Attacking LLMs is inherently risky in security review, while prohibitively expensive. Besides, the continuous iteration of LLMs will degrade the robustness of backdoors. In this paper, we propose TrojanRAG, which employs a joint backdoor attack in the Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack scenarios. Specifically, the adversary constructs elaborate target contexts and trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized by contrastive learning, thus constraining the triggering conditions to a parameter subspace to improve the matching. To improve the recall of the RAG for the target contexts, we introduce a knowledge graph to construct structured data to achieve hard matching at a fine-grained level. Moreover, we normalize the backdoor scenarios in LLMs to analyze the real harm caused by backdoors from both attackers' and users' perspectives and further verify whether the context is a favorable tool for jailbreaking models. Extensive experimental results on truthfulness, language understanding, and harmfulness show that TrojanRAG exhibits versatility threats while maintaining retrieval capabilities on normal queries.},
   author = {Pengzhou Cheng and Yidong Ding and Tianjie Ju and Zongru Wu and Wei Du and Ping Yi and Zhuosheng Zhang and Gongshen Liu},
   month = {5},
   title = {TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models},
   url = {http://arxiv.org/abs/2405.13401},
   year = {2024},
}
@inproceedings{Nam2024,
   abstract = {Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domainspecific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.},
   author = {Daye Nam and Andrew MacVean and Vincent Hellendoorn and Bogdan Vasilescu and Brad Myers},
   doi = {10.1145/3597503.3639187},
   isbn = {9798400702174},
   issn = {02705257},
   booktitle = {Proceedings - International Conference on Software Engineering},
   keywords = {Developer tool,Information support,LLM,Program comprehension,User study},
   pages = {1184-1196},
   publisher = {IEEE Computer Society},
   title = {Using an LLM to Help with Code Understanding},
   year = {2024},
}
@article{Kreimeyer2024,
   abstract = {Modern generative artificial intelligence techniques like retrieval-augmented generation (RAG) may be applied in support of precision oncology treatment discussions. Experts routinely review published literature for evidence and recommendations of treatments in a labor-intensive process. A RAG pipeline may help reduce this effort by providing chunks of text from these publications to an off-the-shelf large language model (LLM), allowing it to answer related questions without any fine-tuning. This potential application is demonstrated by retrieving treatment relationships from a trusted data source (OncoKB) and reproducing over 80% of them by asking simple questions to an untrained Llama 2 model with access to relevant abstracts.},
   author = {Kory Kreimeyer and Jenna V. Canzoniero and Maria Fatteh and Valsamo Anagnostou and Taxiarchis Botsis},
   doi = {10.3233/SHTI240575},
   issn = {18798365},
   journal = {Studies in health technology and informatics},
   keywords = {Large Language Models,Precision Oncology,Retrieval-Augmented Generation},
   month = {8},
   pages = {983-987},
   pmid = {39176956},
   title = {Using Retrieval-Augmented Generation to Capture Molecularly-Driven Treatment Relationships for Precision Oncology},
   volume = {316},
   year = {2024},
}
@misc{du2024vulragenhancingllmbasedvulnerability,
      title={Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG}, 
      author={Xueying Du and Geng Zheng and Kaixin Wang and Jiayi Feng and Wentai Deng and Mingwei Liu and Bihuan Chen and Xin Peng and Tao Ma and Yiling Lou},
      year={2024},
      eprint={2406.11147},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.11147}, 
}

@inproceedings{ WOS:000452649406008,
Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob
   and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin,
   Illia},
Editor = {Guyon, I and Luxburg, UV and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
Title = {Attention Is All You Need},
Booktitle = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)},
Series = {Advances in Neural Information Processing Systems},
Year = {2017},
Volume = {30},
Note = {31st Annual Conference on Neural Information Processing Systems (NIPS),
   Long Beach, CA, DEC 04-09, 2017},
Abstract = {The dominant sequence transduction models are based on complex recurrent
   or convolutional neural networks that include an encoder and a decoder.
   The best performing models also connect the encoder and decoder through
   an attention mechanism. We propose a new simple network architecture,
   the Transformer, based solely on attention mechanisms, dispensing with
   recurrence and convolutions entirely. Experiments on two machine
   translation tasks show these models to be superior in quality while
   being more parallelizable and requiring significantly less time to
   train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German
   translation task, improving over the existing best results, including
   ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation
   task, our model establishes a new single-model state-of-the-art BLEU
   score of 41.0 after training for 3.5 days on eight GPUs, a small
   fraction of the training costs of the best models from the literature.},
Publisher = {NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)},
Address = {10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Vaswani, A (Corresponding Author), Google Brain, Mountain View, CA 94043 USA.
   Vaswani, Ashish; Shazeer, Noam; Kaiser, Lukasz; Polosukhin, Illia, Google Brain, Mountain View, CA 94043 USA.
   Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Polosukhin, Illia, Google Res, Mountain View, CA USA.
   Gomez, Aidan N., Univ Toronto, Toronto, ON, Canada.},
ISSN = {1049-5258},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {avaswani@google.com
   noam@google.com
   nikip@google.com
   usz@google.com
   llion@google.com
   aidan@cs.toronto.edu
   lukaszkaiser@google.com
   illia.polosukhin@gmail.com},
Affiliations = {Google Incorporated; Google Incorporated; University of Toronto},
Cited-References = {{[}Anonymous], 2016, arXiv preprint arXiv:1602.02410.
   {[}Anonymous], Generating sequences with recurrent neural networks.
   Ba JL., 2016, ARXIV.
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473.
   Britz D., 2017, P C EMP METH NAT LAN, P1442.
   Cheng J, 2016, ARXIV160106733, P551, DOI {[}10.18653/v1/d16-1053, DOI 10.18653/V1/D16-1053].
   Cho K, 2014, ARXIV14061078, V2014, P1724, DOI DOI 10.48550/ARXIV.1406.1078.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Chung J., 2014, CORR.
   Gehring J., 2017, ICML, P1243.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI {[}10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2].
   Hochreiter S., 2001, Gradient flow in recurrent nets: The difficulty of learning long-term dependencies, DOI {[}DOI 10.1109/9780470544037.CH14, 10.1109/9780470544037.ch14].
   Kaiser Lukasz, 2016, INT C LEARN REPR.
   Kaiser Samy Bengio Lukasz, 2016, ADV NEURAL INFORM PR.
   Kalchbrenner N, 2017, NEURAL MACHINE TRANS.
   Kim Yoon, 2017, Structured attention networks.
   Kingma D. P., 2014, INT C LEARN REPR.
   Kuchaiev Oleksii., 2017, CoRR.
   Lin Z., 2017, ARXIV170303130, V1703, P03130.
   Luong Minh-Thang, 2015, ARXIV, DOI DOI 10.18653/V1/D15-1166.
   Parikh Ankur, 2016, EMPIRICAL METHODS NA.
   Paulus Romain, 2017, A deep reinforced model for abstractive summarization arXiv preprint arXiv:1705 04304.
   Press O., 2016, ARXIV160805859.
   Sennrich R., 2015, Neural machine translation of rare words with subword units.
   Shazeer Noam, 2017, P 5 INT C LEARN REPR.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Sukhbaatar S, 2015, ADV NEUR IN, V28.
   Sutskever Ilya, 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.5555/2969033.2969173.
   SZEGEDY C, 2016, PROC CVPR IEEE, P2818, DOI {[}10.1109/CVPR.2016.308, DOI 10.1109/CVPR.2016.308].
   Wu Yonghui, 2016, Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.
   Zhou Jie, 2016, ABS160604199 CORR.},
Number-of-Cited-References = {32},
Times-Cited = {53713},
Usage-Count-Last-180-days = {1067},
Usage-Count-Since-2013 = {5542},
Doc-Delivery-Number = {BL5ST},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000452649406008},
DA = {2024-10-06},
}


@inproceedings{ WOS:000900116904035,
Author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova,
   Kristina},
Book-Group-Author = {Assoc Computat Linguist},
Title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
   Understanding},
Booktitle = {2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR
   COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019),
   VOL. 1},
Year = {2019},
Pages = {4171-4186},
Note = {Conference of the North-American-Chapter of the
   Association-for-Computational-Linguistics - Human Language Technologies
   (NAACL-HLT), Minneapolis, MN, JUN 02-07, 2019},
Organization = {Assoc Computat Linguist, N Amer Chapter; Comp Res Assoc, Comp Community
   Consortium; Natl Sci Fdn; Natl Rees Council Canada; Google},
Abstract = {We introduce a new language representation model called BERT, which
   stands for Bidirectional Encoder Representations from Transformers.
   Unlike recent language representation models (Peters et al., 2018a;
   Radford et al., 2018), BERT is designed to pretrain deep bidirectional
   representations from unlabeled text by jointly conditioning on both left
   and right context in all layers. As a result, the pre-trained BERT model
   can be fine-tuned with just one additional output layer to create
   state-of-the-art models for a wide range of tasks, such as question
   answering and language inference, without substantial task-specific
   architecture modifications.
   BERT is conceptually simple and empirically powerful. It obtains new
   state-of-the-art results on eleven natural language processing tasks,
   including pushing the GLUE score to 80.5\% (7.7\% point absolute
   improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement),
   SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute
   improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute
   improvement).},
ISBN = {978-1-950737-13-0},
Unique-ID = {WOS:000900116904035},
}